{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "FederationLearner.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFzoY0mdv2Np"
      },
      "source": [
        "import datetime\n",
        "import importlib\n",
        "from types import SimpleNamespace\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "np.random.seed(0)\n",
        "sns.set()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWNVAQQpOsmM",
        "outputId": "8fb19931-133e-417d-87ac-17211b7632f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%ls\n",
        "# bring the data file csv for"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "buildingDB.csv  \u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YScDMTkL988o"
      },
      "source": [
        "#keras extra\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow.keras as keras\n",
        "import matplotlib.pyplot as plt\n",
        "# from IPython.display import clear_output\n",
        "\n",
        "\n",
        "def plot_history(history):\n",
        "    # Plot training & validation loss values\n",
        "    plt.plot(history[\"loss\"])\n",
        "    plt.plot(history[\"val_loss\"])\n",
        "    plt.title(\"Model loss\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.legend([\"Train\", \"Test\"], loc=\"upper left\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "class PlotLossesBatch(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.i = 0\n",
        "        self.x = []\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "\n",
        "        self.fig = plt.figure()\n",
        "\n",
        "        self.logs = []\n",
        "\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "        if batch == 0 or batch % 50 != 0:\n",
        "            return\n",
        "\n",
        "        self.logs.append(logs)\n",
        "        self.x.append(self.i)\n",
        "        self.losses.append(logs.get(\"loss\"))\n",
        "        self.val_losses.append(logs.get(\"val_loss\"))\n",
        "        self.i += 1\n",
        "\n",
        "        # clear_output(wait=True)\n",
        "\n",
        "        plt.plot(self.x, self.losses, label=\"loss\")\n",
        "        plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "\n",
        "        self.logs.append(logs)\n",
        "        self.x.append(self.i)\n",
        "        self.losses.append(logs.get(\"loss\"))\n",
        "        self.val_losses.append(logs.get(\"val_loss\"))\n",
        "        self.i += 1\n",
        "\n",
        "        # clear_output(wait=True)\n",
        "        plt.plot(self.x, self.losses, label=\"loss\")\n",
        "        plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "class PlotLosses(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.i = 0\n",
        "        self.x = []\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "\n",
        "        self.fig = plt.figure()\n",
        "\n",
        "        self.logs = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "\n",
        "        self.logs.append(logs)\n",
        "        self.x.append(self.i)\n",
        "        self.losses.append(logs.get(\"loss\"))\n",
        "        self.val_losses.append(logs.get(\"val_loss\"))\n",
        "        self.i += 1\n",
        "\n",
        "        # clear_output(wait=True)\n",
        "        plt.plot(self.x, self.losses, label=\"loss\")\n",
        "        plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
        "        plt.legend()\n",
        "        plt.show()\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6INd5o_86UH"
      },
      "source": [
        "# datamunge \n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "def rolling_window(series, window_size):\n",
        "    \"\"\"\n",
        "    Transforms an array of series into an array of sliding window arrays. If\n",
        "    the passed in series is a matrix, each column will be transformed into an\n",
        "    array of sliding windows.\n",
        "    \"\"\"\n",
        "    return np.array(\n",
        "        [\n",
        "            series[i : (i + window_size)]\n",
        "            for i in range(0, series.shape[0] - window_size + 1)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "\n",
        "def multi_shuffle(inputs):\n",
        "    \"\"\"\n",
        "    Shuffles arrays of the same length in lockstep.\n",
        "    \"\"\"\n",
        "    \n",
        "    indices = np.arange(inputs[0].shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    return list(map(lambda x: x[indices], inputs))\n",
        "\n",
        "\n",
        "def dual_shuffle(x, y):\n",
        "    \"\"\"\n",
        "    Shuffles two arrays of the same length in the same positions.\n",
        "    \"\"\"\n",
        "    results = multi_shuffle([x, y])\n",
        "    return results[0], results[1]\n",
        "\n",
        "\n",
        "def to_timeseries_input(series, lookback, predictions, output_col=0):\n",
        "    inputs = rolling_window(series[:-predictions, :], lookback)\n",
        "    outputs = rolling_window(series[lookback:, output_col], predictions)\n",
        "\n",
        "    return inputs, outputs\n",
        "\n",
        "\n",
        "def to_timeseries_by_partition(series, partition, lookback, predictions, output_col=0):\n",
        "    parts = series[partition].unique()\n",
        "\n",
        "    series_x = []\n",
        "    series_y = []\n",
        "\n",
        "    for part in parts:\n",
        "        ds_partition = (\n",
        "            series[series[partition] == part].drop([partition], axis=1).values\n",
        "        )\n",
        "        ts_x, ts_y = to_timeseries_input(\n",
        "            ds_partition, lookback, predictions, output_col\n",
        "        )\n",
        "        series_x.append(ts_x)\n",
        "        series_y.append(ts_y)\n",
        "\n",
        "    return np.concatenate(series_x), np.concatenate(series_y)\n",
        "\n",
        "\n",
        "def normalize_dataset(df, train_split_index):\n",
        "    \"\"\"\n",
        "    Normalizes the given dataframe, using only the train data. The test train\n",
        "    split is decided by the passed in split index.\n",
        "\n",
        "    Args:\n",
        "        df (dataframe): a pandas dataframe to normalize\n",
        "        train_split_index (datetime): a datetime object by which to split the\n",
        "            dataframe\n",
        "    \"\"\"\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(df.iloc[0:train_split_index])\n",
        "    return (scaler, scaler.transform(df))\n",
        "\n",
        "\n",
        "def rescale(scaler, in_y, orig_columns=5):\n",
        "    req_shape = np.dstack([in_y for i in range(0, orig_columns)]).reshape(\n",
        "        in_y.shape[0], orig_columns\n",
        "    )\n",
        "    return scaler.inverse_transform(req_shape)[:, 0]\n",
        "\n",
        "\n",
        "def find_split_index(df, split_date):\n",
        "    \"\"\"\n",
        "    Finds the index of the split point of the dataframe were it to be\n",
        "    split by the passed in date time.\n",
        "\n",
        "    Args:\n",
        "        df (dataframe): a pandas dataframe to split by date\n",
        "        split_date (datetime): a datetime object by which to split the\n",
        "            dataframe\n",
        "    \"\"\"\n",
        "    return len(df[df.index < split_date])\n",
        "\n",
        "\n",
        "def lag_normalize_split(df, split_date, lookback=12, num_predictions=12, output_col=0):\n",
        "    train_split_index = find_split_index(df, split_date)\n",
        "\n",
        "    scaler, scaled = normalize_dataset(df, train_split_index)\n",
        "\n",
        "    train = scaled[0:train_split_index]\n",
        "    test = scaled[train_split_index:]\n",
        "\n",
        "    train_x, train_y = to_timeseries_input(\n",
        "        train, lookback, num_predictions, output_col=output_col\n",
        "    )\n",
        "    # train_x, train_y = dual_shuffle(train_x, train_y)\n",
        "\n",
        "    test_x, test_y = to_timeseries_input(\n",
        "        test, lookback, num_predictions, output_col=output_col\n",
        "    )\n",
        "\n",
        "    return (scaler, train_x, test_x, train_y, test_y)\n",
        "\n",
        "\n",
        "def remove_last_dim(arr):\n",
        "    \"\"\"\n",
        "    Reshapes the given array to remove the last dimension (this makes\n",
        "    the assumption that the last dimension is of shape 1).\n",
        "    \"\"\"\n",
        "    return arr.reshape(arr.shape[0], arr.shape[1])\n",
        "\n",
        "\n",
        "def prepare_inputs_by_partition(\n",
        "    df,\n",
        "    partition_col,\n",
        "    split_date,\n",
        "    categorical_cols=None,\n",
        "    output_col=0,\n",
        "    lookback=12,\n",
        "    num_predictions=12,\n",
        "):\n",
        "    \"\"\"\n",
        "    Lags, splits and normalizes a dataframe based around a partition.\n",
        "    \"\"\"\n",
        "    partitions = df[partition_col].unique()\n",
        "    scalers = {}\n",
        "    train_x = None\n",
        "    test_x = None\n",
        "    train_y = None\n",
        "    test_y = None\n",
        "    testset_by_partition = {}\n",
        "    for partition in partitions:\n",
        "        df_part = df.loc[df[partition_col] == partition].copy()\n",
        "\n",
        "        if categorical_cols is None:\n",
        "            df_cat_train = None\n",
        "            df_cat_test = None\n",
        "        else:\n",
        "            train_split_index = find_split_index(df_part, split_date)\n",
        "            df_cat_train = df_part.iloc[\n",
        "                :train_split_index, categorical_cols\n",
        "            ].values.astype(np.float32)\n",
        "            df_cat_test = df_part.iloc[\n",
        "                train_split_index:, categorical_cols\n",
        "            ].values.astype(np.float32)\n",
        "            df_part.drop(df_part.columns[categorical_cols], axis=1, inplace=True)\n",
        "\n",
        "        df_part.drop([partition_col], axis=1, inplace=True)\n",
        "\n",
        "        scaler, tr_x, te_x, tr_y, te_y = lag_normalize_split(\n",
        "            df_part,\n",
        "            split_date,\n",
        "            output_col=output_col,\n",
        "            lookback=lookback,\n",
        "            num_predictions=num_predictions,\n",
        "        )\n",
        "        scalers[partition] = scaler\n",
        "        \n",
        "        testset_by_partition[partition] = {\n",
        "            \"test_x\": te_x\n",
        "            if df_cat_test is None\n",
        "            else [te_x, df_cat_test[0 : len(te_x)]],\n",
        "            \"test_y\": remove_last_dim(te_y),\n",
        "        }\n",
        "\n",
        "        if train_x is None:\n",
        "            train_x = tr_x\n",
        "            test_x = te_x\n",
        "            train_y = tr_y\n",
        "            test_y = te_y\n",
        "            if not df_cat_train is None:\n",
        "                train_x_cat = df_cat_train[: len(tr_x)]\n",
        "                test_x_cat = df_cat_test[: len(te_x)]\n",
        "        else:\n",
        "            train_x = np.concatenate((train_x, tr_x))\n",
        "            test_x = np.concatenate((test_x, te_x))\n",
        "            train_y = np.concatenate((train_y, tr_y))\n",
        "            test_y = np.concatenate((test_y, te_y))\n",
        "            if not df_cat_train is None:\n",
        "                train_x_cat = np.concatenate((train_x_cat, df_cat_train[: len(tr_x)]))\n",
        "                test_x_cat = np.concatenate((test_x_cat, df_cat_test[: len(te_x)]))\n",
        "\n",
        "    return (\n",
        "        scalers,\n",
        "        train_x if df_cat_train is None else [train_x, train_x_cat],\n",
        "        test_x if df_cat_test is None else [test_x, test_x_cat],\n",
        "        remove_last_dim(train_y),\n",
        "        remove_last_dim(test_y),\n",
        "        testset_by_partition,\n",
        "    )\n",
        "\n",
        "\n",
        "def shuffle_train_sets(train_x, train_y):\n",
        "    if isinstance(train_x, list):\n",
        "        shuffled = multi_shuffle([train_x[0], train_x[1], train_y])\n",
        "        return [shuffled[0], shuffled[1]], shuffled[2]\n",
        "\n",
        "    return dual_shuffle(train_x, train_y)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzOQMl9S9mPc"
      },
      "source": [
        "# date utils.py \n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def make_datetime_index(start_date, end_date=None, freq=\"5min\", periods=None):\n",
        "    return pd.date_range(start=start_date, end=end_date, freq=freq, periods=periods)\n",
        "\n",
        "\n",
        "def make_daterange(start_date, end_date=None, freq=\"5min\", periods=None):\n",
        "    return make_datetime_index(\n",
        "        start_date, end_date, freq, periods=periods\n",
        "    ).to_pydatetime()\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_Fbfly_9rBG"
      },
      "source": [
        "#experiments\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "\n",
        "ALL_ROOMS = [\"babbage\", \"babyfoot\", \"jacquard\"]\n",
        "\n",
        "\n",
        "def print_losses(history):\n",
        "    print(\"{:<25} {:<10}\".format(\"Loss Name\", \"Losses\"))\n",
        "    print(\"\")\n",
        "    for key, item in history.history.items():\n",
        "        print(\"{:<25}\".format(key), end=\"\")\n",
        "        for val in item:\n",
        "            print(\"{:.5f} \".format(val), end=\"\")\n",
        "        print(\"\")\n",
        "\n",
        "\n",
        "def rescaled_mae(scaler, preds, true_vals, orig_columns=5):\n",
        "    \"\"\"\n",
        "    Calculates the MAE after rescaling the data.\n",
        "    \"\"\"\n",
        "    preds_rs = rescale(scaler, preds, orig_columns)\n",
        "    true_vals_rs = rescale(scaler, true_vals, orig_columns)\n",
        "    return mean_absolute_error(true_vals_rs, preds_rs)\n",
        "\n",
        "\n",
        "def print_mae(preds, true_vals):\n",
        "    \"\"\" Prints the MAE for the given prediction and truth values \"\"\"\n",
        "    print(f\"MAE {mean_absolute_error(true_vals, preds)}\")\n",
        "\n",
        "\n",
        "def print_rescaled_mae(scaler, preds, true_vals, orig_columns=5):\n",
        "    print(f\"MAE rescaled {rescaled_mae(scaler, preds, true_vals, orig_columns)}\")\n",
        "\n",
        "\n",
        "def make_predictions(model, test_x, test_y):\n",
        "    preds = model.predict(test_x).flatten()\n",
        "    true_vals = test_y.flatten()\n",
        "    return preds, true_vals\n",
        "\n",
        "\n",
        "def run_predictions(model, in_test_x, in_test_y, scaler=None):\n",
        "    preds, true_vals = make_predictions(model, in_test_x, in_test_y)\n",
        "\n",
        "    print_mae(preds, true_vals)\n",
        "    if not scaler is None:\n",
        "        print_rescaled_mae(scaler, preds, true_vals, in_test_x.shape[2])\n",
        "\n",
        "    return (preds, true_vals)\n",
        "\n",
        "\n",
        "def make_bywindow_predictions(model, test_x, test_y):\n",
        "    if isinstance(test_x, list):\n",
        "        #lookback = test_x[0].shape[1]\n",
        "        predictions = test_y.shape[1]\n",
        "        test_set = [test_x[0][0::predictions], test_x[1][0::predictions]]\n",
        "    else:\n",
        "        #lookback = test_x.shape[1]\n",
        "        predictions = test_y.shape[1]\n",
        "        test_set = test_x[0::predictions]\n",
        "\n",
        "    preds = model.predict(test_set).flatten()\n",
        "    true_vals = test_y[0::predictions].flatten()\n",
        "\n",
        "    return preds, true_vals\n",
        "\n",
        "\n",
        "def make_bywindow_multi_predictions(model, test_x, test_sets):\n",
        "    \"\"\"\n",
        "    Using the model and the given test sets, makes predictions based on the window\n",
        "    used in the train set and joins the predictions together to make one continuous\n",
        "    prediction. This multi prediction version is used for models that have multiple\n",
        "    output predictions.\n",
        "    \"\"\"\n",
        "    if isinstance(test_x, list):\n",
        "        lookback = test_x[0].shape[1]\n",
        "        test_set = [test_x[0][0::lookback], test_x[1][0::lookback]]\n",
        "    else:\n",
        "        lookback = test_x.shape[1]\n",
        "        test_set = test_x[0::lookback]\n",
        "\n",
        "    preds = model.predict(test_set)\n",
        "\n",
        "    preds = [x.flatten() for x in preds]\n",
        "    truth_sets = [x[0::lookback].flatten() for x in test_sets]\n",
        "\n",
        "    return preds, truth_sets\n",
        "\n",
        "\n",
        "def plot_preds_v_truth(preds, true_vals, truth_name=\"temperature\", chart_index=None):\n",
        "    pandas_2line_plot_from_data(\n",
        "        true_vals, preds, truth_name, \"predictions\", chart_index\n",
        "    )\n",
        "\n",
        "\n",
        "def chart_index_for_room(df, split_date, room, num_predictions=12):\n",
        "    test_set = df.loc[df.index >= split_date]\n",
        "    return test_set[test_set.room == room].iloc[:-num_predictions].index\n",
        "\n",
        "\n",
        "def prediction_chart_for_room(\n",
        "    model,\n",
        "    room,\n",
        "    room_testsets,\n",
        "    truth_name=\"temperature\",\n",
        "    chart_index=None,\n",
        "    scaler=None,\n",
        "    orig_columns=5,\n",
        "):\n",
        "    in_test_x = room_testsets[room][\"test_x\"]\n",
        "    in_test_y = room_testsets[room][\"test_y\"]\n",
        "\n",
        "    preds, true_vals = make_bywindow_predictions(model, in_test_x, in_test_y)\n",
        "\n",
        "    if not scaler is None:\n",
        "        preds = rescale(scaler, preds, orig_columns)\n",
        "        true_vals = rescale(scaler, true_vals, orig_columns)\n",
        "\n",
        "    plot_preds_v_truth(\n",
        "        preds, true_vals, truth_name=f\"{truth_name} {room}\", chart_index=chart_index\n",
        "    )\n",
        "\n",
        "\n",
        "def prediction_chart_for_partition(\n",
        "    model,\n",
        "    partition,\n",
        "    testsets,\n",
        "    truth_name=\"temperature\",\n",
        "    chart_index=None,\n",
        "    scaler=None,\n",
        "    orig_columns=5,\n",
        "):\n",
        "    in_test_x = testsets[partition][\"test_x\"]\n",
        "    in_test_y = testsets[partition][\"test_y\"]\n",
        "\n",
        "    preds, true_vals = make_bywindow_predictions(model, in_test_x, in_test_y)\n",
        "\n",
        "    if not scaler is None:\n",
        "        preds = rescale(scaler, preds, orig_columns)\n",
        "        true_vals = rescale(scaler, true_vals, orig_columns)\n",
        "\n",
        "    plot_preds_v_truth(\n",
        "        preds,\n",
        "        true_vals,\n",
        "        truth_name=f\"{truth_name} {partition}\",\n",
        "        chart_index=chart_index,\n",
        "    )\n",
        "\n",
        "\n",
        "def prediction_charts_by_room(\n",
        "    model,\n",
        "    room_testsets,\n",
        "    truth_name=\"temperature\",\n",
        "    chart_index=None,\n",
        "    rooms=None,\n",
        "    scalers=None,\n",
        "    orig_columns=5,\n",
        "):\n",
        "    if rooms is None:\n",
        "        rooms = ALL_ROOMS\n",
        "\n",
        "    for room in rooms:\n",
        "        if scalers is None:\n",
        "            scaler = None\n",
        "        else:\n",
        "            scaler = scalers[room]\n",
        "\n",
        "        prediction_chart_for_room(\n",
        "            model,\n",
        "            room,\n",
        "            room_testsets,\n",
        "            truth_name=truth_name,\n",
        "            chart_index=chart_index,\n",
        "            scaler=scaler,\n",
        "            orig_columns=orig_columns,\n",
        "        )\n",
        "\n",
        "\n",
        "def run_predictions_for_rooms(model, room_testsets, scalers, rooms=None):\n",
        "    if rooms is None:\n",
        "        rooms = ALL_ROOMS\n",
        "\n",
        "    for room in rooms:\n",
        "        in_test_x = room_testsets[room][\"test_x\"]\n",
        "        in_test_y = room_testsets[room][\"test_y\"]\n",
        "\n",
        "        scaler = scalers[room]\n",
        "\n",
        "        print(f\"Predictions for {room}\")\n",
        "        run_predictions(model, in_test_x, in_test_y, scaler)\n",
        "        print(\"\")\n",
        "\n",
        "\n",
        "def replace_column_in_input(pred_vals, in_vals, col):\n",
        "    return np.dstack((np.delete(in_vals, col, axis=2), pred_vals))\n",
        "\n",
        "\n",
        "def get_slice_of_testset(test_x, start_index=0, end_index=1):\n",
        "    if isinstance(test_x, list):\n",
        "        return list(map(lambda x: x[start_index:end_index], test_x))\n",
        "\n",
        "    return test_x[start_index:end_index]\n",
        "\n",
        "\n",
        "def make_stepthrough_predictions(model, test_x, test_y, output_col=0, window_size=12):\n",
        "    test_set = get_slice_of_testset(test_x)\n",
        "\n",
        "    preds = model.predict(test_set).flatten()\n",
        "\n",
        "    if isinstance(test_x, list):\n",
        "        test_length = len(test_x[0])\n",
        "    else:\n",
        "        test_length = len(test_x)\n",
        "\n",
        "    for i in range(window_size, test_length, window_size):\n",
        "        last_prediction = preds[-window_size:]\n",
        "        input_slice = get_slice_of_testset(test_x, i, i + 1)\n",
        "        if isinstance(test_x, list):\n",
        "            new_input = input_slice\n",
        "            new_input[0] = replace_column_in_input(\n",
        "                last_prediction, input_slice[0], output_col\n",
        "            )\n",
        "        else:\n",
        "            new_input = replace_column_in_input(\n",
        "                last_prediction, input_slice, output_col\n",
        "            )\n",
        "        preds = np.append(preds, model.predict(new_input).flatten())\n",
        "\n",
        "    true_vals = test_y.flatten()[0::window_size]\n",
        "    return preds, true_vals\n",
        "\n",
        "\n",
        "def stepthrough_predictions_for_daynum(\n",
        "    model, test_x, test_y, daynum=0, dayindex=0, dayspan=1, output_col=0, window_size=12\n",
        "):\n",
        "    slice_start = 288 * daynum + dayindex\n",
        "    slice_end = slice_start + dayspan * 288\n",
        "    index_slice = slice(slice_start, slice_end)\n",
        "\n",
        "    if isinstance(test_x, list):\n",
        "        test_slice_x = list(map(lambda x: x[index_slice], test_x))\n",
        "    else:\n",
        "        test_slice_x = test_x[index_slice]\n",
        "\n",
        "    test_slice_y = test_y[index_slice]\n",
        "\n",
        "    return make_stepthrough_predictions(\n",
        "        model, test_slice_x, test_slice_y, output_col, window_size\n",
        "    )\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mATW3C8K97Nw"
      },
      "source": [
        "#generators\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def multi_gen(generators, shuffle=False):\n",
        "    \"\"\"\n",
        "    Generator that combines multiple other generators to return samples. Will\n",
        "    either return a value from each generator in succession or randomly\n",
        "    depending on the value of the shuffle parameter.\n",
        "    \"\"\"\n",
        "    i = -1\n",
        "\n",
        "    while 1:\n",
        "        i = np.random.randint(0, 3) if shuffle else (i + 1) % len(generators)\n",
        "        gen = generators[i]\n",
        "        sample = next(gen)\n",
        "        yield sample\n",
        "\n",
        "\n",
        "def ts_generator(\n",
        "    data,\n",
        "    lookback,\n",
        "    target_col=0,\n",
        "    n_outputs=1,\n",
        "    step=1,\n",
        "    min_index=0,\n",
        "    max_index=None,\n",
        "    delay=0,\n",
        "    shuffle=False,\n",
        "    batch_size=16,\n",
        "):\n",
        "    \"\"\"\n",
        "    Generator that creates 3d time series shaped data for use in RNN layers\n",
        "    or similar\n",
        "\n",
        "    Args:\n",
        "        data (array): an indexable matrix of timeseries data\n",
        "        lookback (int): how many timesteps back the input data should go\n",
        "        delay (int): how many steps into the future the target should be\n",
        "        min_index (int): point in data at which to start\n",
        "        max_index (int): point in data at which to finish\n",
        "        shuffle (boolean): whether to shuffle the samples\n",
        "        batch_size (int): the number of samples per batch\n",
        "        step (int): the period in timesteps at which to sample the data\n",
        "    \"\"\"\n",
        "    if max_index is None:\n",
        "        max_index = len(data) - delay - n_outputs\n",
        "\n",
        "    i = min_index + lookback\n",
        "\n",
        "    # if shuffle:\n",
        "    #    np.random.shuffle(data)\n",
        "\n",
        "    while 1:\n",
        "        if shuffle:\n",
        "            rows = np.random.randint(min_index + lookback, max_index, size=batch_size)\n",
        "        else:\n",
        "            if i + batch_size >= max_index:\n",
        "                i = min_index + lookback\n",
        "            rows = np.arange(i, min(i + batch_size, max_index))\n",
        "            i += len(rows)\n",
        "\n",
        "        samples = np.zeros((len(rows), lookback // step, data.shape[-1]))\n",
        "        targets = np.zeros((len(rows), n_outputs))\n",
        "\n",
        "        for j, _ in enumerate(rows):\n",
        "            indices = range(rows[j] - lookback, rows[j], step)\n",
        "            samples[j] = data[indices]\n",
        "            target_start = rows[j] + delay\n",
        "            target_end = target_start + n_outputs\n",
        "            targets[j] = data[target_start:target_end, target_col]\n",
        "\n",
        "        if n_outputs == 1:\n",
        "            targets = targets.reshape(targets.shape[:-1])\n",
        "\n",
        "        yield samples, targets, [None]\n",
        "\n",
        "\n",
        "def ts_seasonal_generator(\n",
        "    data,\n",
        "    target_col=0,\n",
        "    block_size=24,\n",
        "    n_outputs=12,\n",
        "    step=1,\n",
        "    min_index=0,\n",
        "    max_index=None,\n",
        "    delay=0,\n",
        "    shuffle=False,\n",
        "    batch_size=16,\n",
        "    freq=5,\n",
        "):\n",
        "    \"\"\"\n",
        "    Generator that creates 3d time series shaped data for use in RNN layers\n",
        "    or similar\n",
        "\n",
        "    Args:\n",
        "        data (array): an indexable matrix of timeseries data\n",
        "        lookback (int): how many timesteps back the input data should go\n",
        "        delay (int): how many steps into the future the target should be\n",
        "        min_index (int): point in data at which to start\n",
        "        max_index (int): point in data at which to finish\n",
        "        shuffle (boolean): whether to shuffle the samples\n",
        "        batch_size (int): the number of samples per batch\n",
        "        step (int): the period in timesteps at which to sample the data\n",
        "    \"\"\"\n",
        "    half_sample = block_size // 2\n",
        "    lookback = (60 // freq) * 24 * 7 + half_sample\n",
        "    lookback_d = (60 // freq) * 24 + half_sample\n",
        "\n",
        "    if max_index is None:\n",
        "        max_index = len(data) - delay - n_outputs\n",
        "\n",
        "    i = min_index + lookback\n",
        "\n",
        "    while 1:\n",
        "        if shuffle:\n",
        "            rows = np.random.randint(min_index + lookback, max_index, size=batch_size)\n",
        "        else:\n",
        "            if i + batch_size >= max_index:\n",
        "                i = min_index + lookback\n",
        "            rows = np.arange(i, min(i + batch_size, max_index))\n",
        "            i += len(rows)\n",
        "\n",
        "        samples = np.zeros((len(rows), block_size // step, data.shape[-1] * 3))\n",
        "        targets = np.zeros((len(rows), n_outputs))\n",
        "\n",
        "        for j, _ in enumerate(rows):\n",
        "            indices1 = range(rows[j] - block_size, rows[j], step)\n",
        "            indices2 = range(rows[j] - lookback, rows[j] - lookback + block_size, step)\n",
        "            indices3 = range(rows[j] - lookback_d, rows[j] - lookback_d + block_size, step)\n",
        "            data1 = data[indices1]\n",
        "            data2 = data[indices2]\n",
        "            data3 = data[indices3]\n",
        "            all_data = np.hstack((data1, data2, data3))\n",
        "\n",
        "            #print(samples.shape)\n",
        "            #print(data1.shape, data2.shape, data3.shape)\n",
        "            #print(all_data.shape)\n",
        "            samples[j] = all_data\n",
        "\n",
        "            target_start = rows[j] + delay\n",
        "            target_end = target_start + n_outputs\n",
        "            targets[j] = data[target_start:target_end, target_col]\n",
        "\n",
        "        if n_outputs == 1:\n",
        "            targets = targets.reshape(targets.shape[:-1])\n",
        "\n",
        "        yield samples, targets, [None]\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTzPtNZlZZ0p"
      },
      "source": [
        "!pip install keras-tcn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAc8cQRY-Agn",
        "outputId": "6d32763a-0d80-4422-bb58-003dd96a047f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#keras model\n",
        "\n",
        "import datetime\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import callbacks as kcb\n",
        "from tensorflow.keras.layers import (\n",
        "    GRU,\n",
        "    LSTM,\n",
        "    Conv1D,\n",
        "    Dense,\n",
        "    Flatten,\n",
        "    MaxPooling1D,\n",
        "    RepeatVector,\n",
        ")\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tcn import TCN\n",
        "\n",
        "from tqdm.keras import TqdmCallback\n",
        "\n",
        "\n",
        "def get_tb_logdir():\n",
        "    return os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "\n",
        "\n",
        "def learning_rate_finder_train(model, train_x, train_y, loss=\"mae\", epochs=10):\n",
        "    lr_schedule = keras.callbacks.LearningRateScheduler(\n",
        "        lambda epoch: 1e-8 * 10 ** (epoch / 20)\n",
        "    )\n",
        "    optimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\n",
        "    model.compile(loss=loss, optimizer=optimizer)\n",
        "    history = model.fit(\n",
        "        train_x,\n",
        "        train_y,\n",
        "        callbacks=[lr_schedule, TqdmCallback(verbose=1)],\n",
        "        epochs=epochs,\n",
        "        verbose=0,\n",
        "    )\n",
        "    plot_learning_rate_history(history)\n",
        "    return history\n",
        "\n",
        "\n",
        "def train_model(\n",
        "    model,\n",
        "    train_x,\n",
        "    train_y,\n",
        "    test_x,\n",
        "    test_y,\n",
        "    use_tensorboard=False,\n",
        "    live_valchart=False,\n",
        "    epochs=10,\n",
        "    batch_size=16,\n",
        "    verbose=0,\n",
        "    early_stopping_patience=4,\n",
        "):\n",
        "    callbacks = [TqdmCallback(verbose=1)]\n",
        "\n",
        "    if early_stopping_patience > 0:\n",
        "        callbacks.append(\n",
        "            keras.callbacks.EarlyStopping(patience=4, restore_best_weights=True)\n",
        "        )\n",
        "\n",
        "    if use_tensorboard:\n",
        "        logdir = get_tb_logdir()\n",
        "        tensorboard_callback = kcb.TensorBoard(\n",
        "            logdir, histogram_freq=1, profile_batch=0\n",
        "        )\n",
        "        callbacks.append(tensorboard_callback)\n",
        "\n",
        "    if live_valchart:\n",
        "        plot_losses = PlotLosses()\n",
        "        callbacks.append(plot_losses)\n",
        "\n",
        "    return model.fit(\n",
        "        train_x,\n",
        "        train_y,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        verbose=verbose,\n",
        "        validation_data=(test_x, test_y),\n",
        "        callbacks=callbacks,\n",
        "    )\n",
        "\n",
        "\n",
        "def get_rnn_shapes(x, y):\n",
        "    return (\n",
        "        x.shape[1],\n",
        "        x.shape[2],\n",
        "        y.shape[1],\n",
        "    )\n",
        "\n",
        "\n",
        "def build_model(\n",
        "    model_func,\n",
        "    train_x,\n",
        "    train_y,\n",
        "    test_x,\n",
        "    test_y,\n",
        "    epochs=15,\n",
        "    batch_size=16,\n",
        "    use_tensorboard=False,\n",
        "):\n",
        "    n_timesteps, n_features, n_outputs = get_rnn_shapes(train_x, train_y)\n",
        "\n",
        "    verbose = 1\n",
        "    input_shape = (n_timesteps, n_features)\n",
        "\n",
        "    # define model\n",
        "    model = model_func(input_shape, n_outputs)\n",
        "\n",
        "    out_shape = model.layers[-1].output_shape[1:]\n",
        "    train_y = train_y.reshape((train_y.shape[0],) + out_shape)\n",
        "    test_y = test_y.reshape((test_y.shape[0],) + out_shape)\n",
        "\n",
        "    return train_model(\n",
        "        model,\n",
        "        train_x,\n",
        "        train_y,\n",
        "        test_x,\n",
        "        test_y,\n",
        "        use_tensorboard=use_tensorboard,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        verbose=verbose,\n",
        "    )\n",
        "\n",
        "\n",
        "def model_encoderdecoder(input_shape, n_outputs, activation=\"relu\", loss=\"mae\"):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(100, activation=activation, input_shape=input_shape))\n",
        "    model.add(RepeatVector(n_outputs))\n",
        "    model.add(LSTM(100, activation=activation, return_sequences=True))\n",
        "    model.add(Dense(100, activation=activation))\n",
        "    model.add(Dense(1, activation=activation))\n",
        "    model.compile(loss=loss, optimizer=\"adam\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def model_encoderdecoder_tanh(input_shape, n_outputs, loss=\"mae\"):\n",
        "    return model_encoderdecoder(input_shape, n_outputs, activation=\"tanh\", loss=loss)\n",
        "\n",
        "\n",
        "def model_convnet(input_shape, n_outputs, activation=\"relu\", loss=\"mae\"):\n",
        "    model = Sequential()\n",
        "    model.add(\n",
        "        Conv1D(\n",
        "            filters=32, kernel_size=3, activation=activation, input_shape=input_shape\n",
        "        )\n",
        "    )\n",
        "    model.add(Conv1D(filters=32, kernel_size=3, activation=activation))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Conv1D(filters=16, kernel_size=3, activation=activation))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(100, activation=activation))\n",
        "    model.add(Dense(n_outputs))\n",
        "    model.compile(loss=loss, optimizer=\"adam\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def model_simple_lstm(input_shape, n_outputs, loss=\"mae\"):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(50, input_shape=input_shape))\n",
        "    model.add(Dense(n_outputs))\n",
        "    model.compile(loss=loss, optimizer=\"adam\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def model_simple_gru(input_shape, n_outputs, loss=\"mae\", optimizer=\"adam\"):\n",
        "    model = Sequential()\n",
        "    model.add(GRU(32, input_shape=input_shape))\n",
        "    model.add(Dense(n_outputs))\n",
        "    model.compile(loss=loss, optimizer=optimizer)\n",
        "    return model\n",
        "\n",
        "\n",
        "def model_simple_tcn(input_shape, n_outputs, loss=\"mae\", optimizer=\"adam\"):\n",
        "    i = keras.layers.Input(shape=input_shape)\n",
        "\n",
        "    x = TCN(return_sequences=False)(i)\n",
        "    x = keras.layers.Dense(n_outputs)(x)\n",
        "\n",
        "    model = keras.Model(i, x)\n",
        "    model.compile(optimizer=optimizer, loss=loss)\n",
        "    return model\n",
        "\n",
        "\n",
        "def model_gru_conv(input_shape, n_outputs, loss=\"mae\", optimizer=\"adam\"):\n",
        "    inp = keras.layers.Input(shape=input_shape)\n",
        "    x = keras.layers.GRU(32, return_sequences=True)(inp)\n",
        "    x = keras.layers.AveragePooling1D(2)(x)\n",
        "    x = keras.layers.Conv1D(32, 3, activation=\"relu\", padding=\"same\", name=\"f_extract\")(\n",
        "        x\n",
        "    )\n",
        "    x = keras.layers.Flatten()(x)\n",
        "    x = keras.layers.Dense(16, activation=\"relu\")(x)\n",
        "    out = keras.layers.Dense(n_outputs)(x)\n",
        "\n",
        "    model = keras.models.Model(inp, out)\n",
        "    model.compile(optimizer=optimizer, loss=loss)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def model_conditioned_gru(n_outputs, train_x, loss=\"mae\", optimizer=\"adam\"):\n",
        "    model = ConditionedGRU(num_outputs=n_outputs)\n",
        "    # model.call(train_x)\n",
        "    model.compile(loss=loss, optimizer=optimizer)\n",
        "    return model\n",
        "\n",
        "\n",
        "def model_conditioned_lstm(n_outputs, train_x, loss=\"mae\", optimizer=\"adam\"):\n",
        "    model = ConditionedLSTM(num_outputs=n_outputs)\n",
        "    model.call(train_x)\n",
        "    model.compile(loss=loss, optimizer=optimizer)\n",
        "    return model\n",
        "\n",
        "\n",
        "class ConditionedLSTM(keras.Model):\n",
        "    def __init__(self, num_outputs):\n",
        "        super(ConditionedLSTM, self).__init__()\n",
        "        self.cond = ConditionalRNNLayer(\n",
        "            32, cell=\"LSTM\", return_sequences=True, dtype=tf.float32\n",
        "        )\n",
        "        self.lstm2 = keras.layers.LSTM(24, return_sequences=False)\n",
        "        self.out = keras.layers.Dense(num_outputs)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        x = self.cond(inputs)\n",
        "        x = self.lstm2(x)\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConditionedGRU(keras.Model):\n",
        "    def __init__(self, num_outputs):\n",
        "        super(ConditionedGRU, self).__init__()\n",
        "        self.cond = ConditionalRNNLayer(\n",
        "            32, cell=\"GRU\", return_sequences=False, dtype=tf.float32\n",
        "        )\n",
        "        self.out = keras.layers.Dense(num_outputs)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        o = self.cond(inputs)\n",
        "        o = self.out(o)\n",
        "        return o\n",
        "\n",
        "\n",
        "class ConditionalRNNLayer(keras.layers.Layer):\n",
        "    # Arguments to the RNN like return_sequences, return_state...\n",
        "    def __init__(self, units, cell=keras.layers.LSTMCell, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Conditional RNN. Conditions time series on categorical data.\n",
        "        :param units: int, The number of units in the RNN Cell\n",
        "        :param cell: string, cell class or object (pre-instantiated). In the case of string, 'GRU',\n",
        "        'LSTM' and 'RNN' are supported.\n",
        "        :param args: Any parameters of the keras.layers.RNN class, such as return_sequences,\n",
        "        return_state, stateful, unroll...\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.units = units\n",
        "        self.final_states = None\n",
        "        self.init_state = None\n",
        "        if isinstance(cell, str):\n",
        "            if cell.upper() == \"GRU\":\n",
        "                cell = keras.layers.GRUCell\n",
        "            elif cell.upper() == \"LSTM\":\n",
        "                cell = keras.layers.LSTMCell\n",
        "            elif cell.upper() == \"RNN\":\n",
        "                cell = keras.layers.SimpleRNNCell\n",
        "            else:\n",
        "                raise Exception(\"Only GRU, LSTM and RNN are supported as cells.\")\n",
        "        self._cell = cell if hasattr(cell, \"units\") else cell(units=units)\n",
        "        self.rnn = keras.layers.RNN(cell=self._cell, *args, **kwargs)\n",
        "\n",
        "        # single cond\n",
        "        self.cond_to_init_state_dense_1 = keras.layers.Dense(units=self.units)\n",
        "\n",
        "        # multi cond\n",
        "        max_num_conditions = 10\n",
        "        self.multi_cond_to_init_state_dense = []\n",
        "        for _ in range(max_num_conditions):\n",
        "            self.multi_cond_to_init_state_dense.append(\n",
        "                keras.layers.Dense(units=self.units)\n",
        "            )\n",
        "        self.multi_cond_p = keras.layers.Dense(1, activation=None, use_bias=True)\n",
        "\n",
        "    def _standardize_condition(self, initial_cond):\n",
        "        initial_cond_shape = initial_cond.shape\n",
        "        if len(initial_cond_shape) == 2:\n",
        "            initial_cond = tf.expand_dims(initial_cond, axis=0)\n",
        "        first_cond_dim = initial_cond.shape[0]\n",
        "        if isinstance(self._cell, keras.layers.LSTMCell):\n",
        "            if first_cond_dim == 1:\n",
        "                initial_cond = tf.tile(initial_cond, [2, 1, 1])\n",
        "            elif first_cond_dim != 2:\n",
        "                raise Exception(\n",
        "                    \"Initial cond should have shape: [2, batch_size, hidden_size]\\n\"\n",
        "                    \"or [batch_size, hidden_size]. Shapes do not match.\",\n",
        "                    initial_cond_shape,\n",
        "                )\n",
        "        elif isinstance(self._cell, keras.layers.GRUCell) or isinstance(\n",
        "            self._cell, keras.layers.SimpleRNNCell\n",
        "        ):\n",
        "            if first_cond_dim != 1:\n",
        "                raise Exception(\n",
        "                    \"Initial cond should have shape: [1, batch_size, hidden_size]\\n\"\n",
        "                    \"or [batch_size, hidden_size]. Shapes do not match.\",\n",
        "                    initial_cond_shape,\n",
        "                )\n",
        "        else:\n",
        "            raise Exception(\"Only GRU, LSTM and RNN are supported as cells.\")\n",
        "        return initial_cond\n",
        "\n",
        "    def __call__(self, inputs, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        :param inputs: List of n elements:\n",
        "                    - [0] 3-D Tensor with shape [batch_size, time_steps, input_dim]. The inputs.\n",
        "                    - [1:] list of tensors with shape [batch_size, cond_dim]. The conditions.\n",
        "        In the case of a list, the tensors can have a different cond_dim.\n",
        "        :return: outputs, states or outputs (if return_state=False)\n",
        "        \"\"\"\n",
        "        assert isinstance(inputs, list) and len(inputs) >= 2\n",
        "        x = inputs[0]\n",
        "        cond = inputs[1:]\n",
        "        if len(cond) > 1:  # multiple conditions.\n",
        "            init_state_list = []\n",
        "            for ii, c in enumerate(cond):\n",
        "                init_state_list.append(\n",
        "                    self.multi_cond_to_init_state_dense[ii](\n",
        "                        self._standardize_condition(c)\n",
        "                    )\n",
        "                )\n",
        "            multi_cond_state = self.multi_cond_p(tf.stack(init_state_list, axis=-1))\n",
        "            multi_cond_state = tf.squeeze(multi_cond_state, axis=-1)\n",
        "            self.init_state = tf.unstack(multi_cond_state, axis=0)\n",
        "        else:\n",
        "            cond = self._standardize_condition(cond[0])\n",
        "            if cond is not None:\n",
        "                self.init_state = self.cond_to_init_state_dense_1(cond)\n",
        "                self.init_state = tf.unstack(self.init_state, axis=0)\n",
        "        out = self.rnn(x, initial_state=self.init_state, *args, **kwargs)\n",
        "        if self.rnn.return_state:\n",
        "            outputs, h, c = out\n",
        "            final_states = tf.stack([h, c])\n",
        "            return outputs, final_states\n",
        "\n",
        "        return out\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hCbgG5t-Dls"
      },
      "source": [
        "#plots.py\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def history_plot(history, loss_units=None):\n",
        "    epochs = len(next(iter(history.history.values())))\n",
        "    ylabel = \"loss\" if loss_units is None else f\"loss ({loss_units})\"\n",
        "    xlabel = \"epoch\"\n",
        "    multiline_plot(\n",
        "        history.history,\n",
        "        range(1, epochs + 1),\n",
        "        ticks_from_x=True,\n",
        "        ylabel=ylabel,\n",
        "        xlabel=xlabel,\n",
        "    )\n",
        "\n",
        "\n",
        "def plot_learning_rate_history(history):\n",
        "    \"\"\"\n",
        "    Uses the history from a model trained with a LearningRateScheduler and plots a chart\n",
        "    to help pick the optimal learning rate.\n",
        "    \"\"\"\n",
        "    plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\n",
        "    plt.axis([1e-8, 1e-4, 0, 30])\n",
        "\n",
        "\n",
        "def pandas_multiline_plot_from_data(\n",
        "    data, chart_index=None, figsize=(19, 6),\n",
        "):\n",
        "    \"\"\"\n",
        "    Takes a passed in dictionary of data names and data, and plots the lines side by side,\n",
        "    using the pandas plot functionality. The x axis can be configured by passing in a pandas\n",
        "    index in the chart_index parameter.\n",
        "    \"\"\"\n",
        "    to_plot = pd.DataFrame(data)\n",
        "    if not chart_index is None:\n",
        "        data_len = len(next(iter(data.values())))\n",
        "        to_plot.index = chart_index[0:data_len]\n",
        "\n",
        "    to_plot.plot(figsize=figsize)\n",
        "\n",
        "\n",
        "def pandas_multiline_date_plot(\n",
        "    data, start_date, freq=\"5min\", date_filter=None, figsize=(19, 6),\n",
        "):\n",
        "    \"\"\"\n",
        "    Takes a passed in dictionary of data names and data, and plots the lines side by side,\n",
        "    using the pandas plot functionality. The x axis is formed by creating a pandas datetime\n",
        "    index starting from the parameter start_date. A date filter can be specified to plot\n",
        "    only a portion of the data by indexing the pandas index (this should be a string or a\n",
        "    slice).\n",
        "    \"\"\"\n",
        "    to_plot = pd.DataFrame(data)\n",
        "    data_len = len(next(iter(data.values())))\n",
        "\n",
        "    date_index = make_datetime_index(start_date, freq=freq, periods=data_len)\n",
        "    to_plot.index = date_index\n",
        "\n",
        "    if not date_filter is None:\n",
        "        to_plot = to_plot.loc[date_filter]\n",
        "\n",
        "    to_plot.plot(figsize=figsize)\n",
        "\n",
        "\n",
        "def pandas_2line_plot_from_data(\n",
        "    data1,\n",
        "    data2,\n",
        "    data1_name=\"Data 1\",\n",
        "    data2_name=\"Data 2\",\n",
        "    chart_index=None,\n",
        "    figsize=(19, 6),\n",
        "):\n",
        "    \"\"\"\n",
        "    Takes 2 passed in iterables of data, and plots them side by side, using the pandas\n",
        "    plot functionality. The x axis can be configured by passing in a pandas index in the\n",
        "    chart_index parameter.\n",
        "    \"\"\"\n",
        "    data = {data1_name: data1, data2_name: data2}\n",
        "    pandas_multiline_plot_from_data(data, chart_index=chart_index, figsize=figsize)\n",
        "\n",
        "\n",
        "def twoline_plot(\n",
        "    data1,\n",
        "    data2,\n",
        "    data1_name=\"Data 1\",\n",
        "    data2_name=\"Data 2\",\n",
        "    x_axis=None,\n",
        "    figsize=(19, 6),\n",
        "):\n",
        "    if x_axis is None:\n",
        "        x = range(len(data1))\n",
        "        x_min = 0\n",
        "        x_max = len(x)\n",
        "    else:\n",
        "        x = x_axis[0 : len(data1)]\n",
        "        x_min = min(x)\n",
        "        x_max = max(x)\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.xlim(x_min, x_max)\n",
        "    plt.plot(x, data1, label=data1_name)\n",
        "    plt.plot(x, data2, label=data2_name)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def multiline_plot(\n",
        "    data,\n",
        "    x_axis=None,\n",
        "    figsize=(19, 6),\n",
        "    ticks_from_x=False,\n",
        "    xlabel=None,\n",
        "    ylabel=None,\n",
        "    title=None,\n",
        "):\n",
        "    data_len = len(next(iter(data.values())))\n",
        "    if x_axis is None:\n",
        "        x = range(data_len)\n",
        "        x_min = 0\n",
        "        x_max = len(x) - 1\n",
        "    else:\n",
        "        x = x_axis[0:data_len]\n",
        "        x_min = min(x)\n",
        "        x_max = max(x)\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.xlim(x_min, x_max)\n",
        "    if x_axis is None or ticks_from_x:\n",
        "        plt.xticks(range(1, data_len))\n",
        "\n",
        "    for key, value in data.items():\n",
        "        plt.plot(x, value, label=key)\n",
        "\n",
        "    if not xlabel is None:\n",
        "        plt.xlabel(xlabel)\n",
        "    if not ylabel is None:\n",
        "        plt.ylabel(ylabel)\n",
        "    if not title is None:\n",
        "        plt.title(title)\n",
        "\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def multiline_date_plot(\n",
        "    data, date_start, figsize=(19, 6), freq=\"5min\",\n",
        "):\n",
        "    data_len = len(next(iter(data.values())))\n",
        "    x_axis = make_daterange(date_start, periods=data_len, freq=freq)\n",
        "    multiline_plot(data, x_axis, figsize)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gulyeGJq-JPq"
      },
      "source": [
        "# data set tf extra\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def windowed_dataset(\n",
        "    series, window_size, prediction_size, batch_size, pred_column=0, shuffle=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Creates a windowed dataset using tf datasets\n",
        "    \"\"\"\n",
        "    series = tf.expand_dims(series, axis=-1)\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(window_size + prediction_size, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size + prediction_size + 1))\n",
        "    if shuffle:\n",
        "        shuffle_buffer = 1000\n",
        "        ds = ds.shuffle(shuffle_buffer)\n",
        "    ds = ds.map(\n",
        "        lambda w: (\n",
        "            tf.reshape(w[:window_size], (window_size, series.shape[1])),\n",
        "            tf.reshape(w[-prediction_size:, pred_column], (prediction_size,)),\n",
        "        )\n",
        "    )\n",
        "    return ds.batch(batch_size).prefetch(1)\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siKiIPdkv2Nw"
      },
      "source": [
        "# Some global parameters\n",
        "batch_size = 16\n",
        "lookback = 36\n",
        "n_outputs = 12\n",
        "n_features = 5\n",
        "EPOCHS = 1\n",
        "\n",
        "room_names = [\"babbage\", \"babyfoot\", \"jacquard\"]\n",
        "partitions = list(range(3))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V66SfxVUv2N1"
      },
      "source": [
        "def simple_lstm_compile(model):\n",
        "    model.compile(loss=\"mae\", optimizer=\"adam\")\n",
        "    \n",
        "def simple_lstm_finetune_compile(model):\n",
        "    import tensorflow.keras as keras\n",
        "    opt = keras.optimizers.Adam(1e-5)\n",
        "    \n",
        "    model.compile(loss=\"mae\", optimizer=opt)\n",
        "    \n",
        "def model_simple_lstm(input_shape=(lookback, n_features), n_outputs=12, loss=\"mae\"):\n",
        "    model = keras.models.Sequential()\n",
        "    model.add(keras.layers.LSTM(16, input_shape=input_shape, return_sequences=True))\n",
        "    model.add(keras.layers.LSTM(16, input_shape=input_shape))\n",
        "    model.add(keras.layers.Dense(n_outputs))\n",
        "    # optimizer = keras.optimizers.Adam()\n",
        "    opt = keras.optimizers.Adam(0.0001)\n",
        "    model.compile(loss=loss, optimizer=opt)\n",
        "    return model\n",
        "\n",
        "def model_simple_lstm3(input_shape=(lookback, n_features), n_outputs=12, loss=\"mae\"):\n",
        "    model = keras.models.Sequential()\n",
        "    model.add(keras.layers.LSTM(16, input_shape=input_shape, return_sequences=True))\n",
        "    model.add(keras.layers.LSTM(16, input_shape=input_shape))\n",
        "    model.add(keras.layers.Dense(10))\n",
        "    model.add(keras.layers.Dense(n_outputs))\n",
        "    # optimizer = keras.optimizers.Adam()\n",
        "    opt = keras.optimizers.Adam(0.0001)\n",
        "    model.compile(loss=loss, optimizer=opt)\n",
        "    return model\n",
        "\n",
        "def model_simple_lstm2(input_shape=(lookback, n_features), n_outputs=12, loss=\"mae\"):\n",
        "    model = keras.models.Sequential()\n",
        "    model.add(keras.layers.LSTM(32, input_shape=input_shape, return_sequences=True))\n",
        "    model.add(keras.layers.LSTM(32, input_shape=input_shape, return_sequences=True))\n",
        "    model.add(keras.layers.LSTM(16, input_shape=input_shape, return_sequences=True))\n",
        "    model.add(keras.layers.LSTM(16, input_shape=input_shape))\n",
        "    model.add(keras.layers.Dense(n_outputs))\n",
        "    # optimizer = keras.optimizers.Adam()\n",
        "    opt = keras.optimizers.Adam(0.00005)\n",
        "    model.compile(loss=loss, optimizer=opt)\n",
        "    return model\n",
        "\n",
        "def basic_1dconv(input_shape=(lookback, n_features), n_outputs=12):\n",
        "    from tensorflow.keras.layers import Conv1D, Dense, Flatten, MaxPooling1D\n",
        "    from tensorflow.keras.models import Sequential\n",
        "\n",
        "    activation = \"relu\"\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(\n",
        "        Conv1D(\n",
        "            filters=32,\n",
        "            kernel_size=3,\n",
        "            activation=activation,\n",
        "            input_shape=input_shape,\n",
        "            name=\"layer1\",\n",
        "        ),\n",
        "    )\n",
        "    model.add(Conv1D(filters=32, kernel_size=3, activation=activation, name=\"layer2\"))\n",
        "    model.add(MaxPooling1D(pool_size=2, name=\"layer3\"))\n",
        "    model.add(Conv1D(filters=16, kernel_size=3, activation=activation, name=\"layer4\"))\n",
        "    model.add(MaxPooling1D(pool_size=2, name=\"layer5\"))\n",
        "    model.add(Flatten(name=\"layer6\"))\n",
        "    model.add(Dense(100, activation=activation, name=\"layer7\"))\n",
        "    model.add(Dense(n_outputs, name=\"layer8\"))\n",
        "    opt = keras.optimizers.Adam(0.00005)\n",
        "    model.compile(loss=\"mae\", optimizer=opt)\n",
        "    return model"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VuCgv3Gv2N4"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\n",
        "    \"buildingDB.csv\", \n",
        "    # parse_dates=[\"time\"],\n",
        "    index_col=[\"time\"],\n",
        "    error_bad_lines=False\n",
        "    )\n",
        "df_main = df.drop([\"presence\"], axis=1)\n",
        "# Careful with the following assumption. It worked for Jan, March\n",
        "# to set missing sensor values to zero, but is it still good for march-aug?\n",
        "df_main = df_main.fillna(df_main.min(),) \n",
        "\n",
        "def filter_room(df, room):\n",
        "    return df.loc[df.room == room].drop([\"room\"], axis=1)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6qSYyBQv2N7"
      },
      "source": [
        "class QarnotDataReader:\n",
        "    def __init__(self, gen_val=False, dataset=df_main, split_date='2020-03-08'):\n",
        "\n",
        "        df = dataset.copy()\n",
        "        self.split_date = split_date\n",
        "\n",
        "        split_index = find_split_index(df, self.split_date)\n",
        "        means = np.mean(df.iloc[:split_index, 1:])\n",
        "        stds = np.std(df.iloc[:split_index, 1:])\n",
        "\n",
        "        df.iloc[:, 1:] = (df.iloc[:, 1:] - means) / stds\n",
        "\n",
        "        self.names = room_names\n",
        "        self.df = df\n",
        "        self.scalers = {}\n",
        "        self.gen_val = gen_val\n",
        "        self.lookback = lookback\n",
        "        self.n_outputs = n_outputs\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "\n",
        "    def get_number_of_partitions(self):\n",
        "        return len(self.names)\n",
        "\n",
        "    def get_partitions(self):\n",
        "        return self.names\n",
        "\n",
        "    def get_partition_name(self, pid):\n",
        "        return self.names[pid]\n",
        "\n",
        "    def get_data_for_partition(self, partition):\n",
        "        partition_name = self.get_partition_name(partition)\n",
        "\n",
        "        return self.get_windowed_data_for_partition(\n",
        "            partition_name, self.lookback, self.n_outputs, self.batch_size\n",
        "        )\n",
        "    \n",
        "    def get_general_test_set(self):\n",
        "        \n",
        "        (\n",
        "            _,\n",
        "            _1,\n",
        "            test_x,\n",
        "            _2,\n",
        "            test_y,\n",
        "            _3,\n",
        "        ) = prepare_inputs_by_partition(\n",
        "            self.df, \"room\", self.split_date, output_col=0, lookback=self.lookback, \n",
        "            num_predictions=self.n_outputs,\n",
        "        )\n",
        "        \n",
        "        return test_x, test_y\n",
        "\n",
        "\n",
        "    def get_windowed_data_for_partition(\n",
        "        self, partition, lookback, n_outputs, batch_size\n",
        "    ):\n",
        "\n",
        "        data = self.df[self.df.room == partition].drop([\"room\"], axis=1)\n",
        "        split_index = find_split_index(data, self.split_date)\n",
        "        scaler, norm_data = normalize_dataset(data, split_index)\n",
        "\n",
        "        self.scalers[partition] = scaler\n",
        "\n",
        "        train_data = windowed_dataset(\n",
        "            norm_data[0:split_index], lookback, n_outputs, batch_size\n",
        "        )\n",
        "\n",
        "        if self.gen_val:\n",
        "            return get_general_test_set()\n",
        "        else:\n",
        "            val_data = windowed_dataset(\n",
        "                norm_data[split_index:], lookback, n_outputs, batch_size\n",
        "            )\n",
        "\n",
        "        return (train_data, val_data)\n",
        "\n",
        "\n",
        "qarnot_provider = lambda: QarnotDataReader()\n",
        "qarnot_provider_genval = lambda: QarnotDataReader(gen_val=True)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hEArvPLv2OC"
      },
      "source": [
        "def sort_tuple(name):\n",
        "    parts = str(name).split(\"_\")\n",
        "    parts = list([int(p) if p.isdigit() else p for p in parts])\n",
        "    return (parts[0], parts[0] if len(parts) < 2 else parts[1], name)\n",
        "\n",
        "\n",
        "def sorted_names(name_list):\n",
        "    return list(map(lambda x: x[-1], sorted(map(sort_tuple, name_list))))\n",
        "\n",
        "\n",
        "def line_chart(ds: pd.DataFrame, round_column: str = \"loss\", labels=None):\n",
        "    margins = 0.0\n",
        "\n",
        "    # fig = plt.figure(figsize=(6, 4))  # a new figure window, figsize goes here\n",
        "    fig = plt.figure(figsize=(8, 6))  # a new figure window, figsize goes here\n",
        "    ax = fig.add_subplot(1, 1, 1)  # specify (nrows, ncols, axnum)\n",
        "\n",
        "    dsg = ds.groupby(\"partition\")\n",
        "    legends = []\n",
        "    i = 0\n",
        "    for name, group in dsg:\n",
        "        label = labels[i] if labels else name\n",
        "        legends.append(label)\n",
        "        ax.plot(group[\"epoch\"], group[round_column])\n",
        "        i += 1\n",
        "\n",
        "    # decorate then sort\n",
        "    #legends = sorted_names(legends)\n",
        "\n",
        "    if len(legends) > 4:\n",
        "        ax.legend(legends[0 : min(12, len(legends))], fancybox=True, framealpha=0.5)\n",
        "    else:\n",
        "        ax.legend(legends)\n",
        "    ax.set_xlabel(\"Training epoch\")\n",
        "    ax.set_ylabel(round_column)\n",
        "    ax.set_title(f\"Metric {round_column}\")\n",
        "\n",
        "    ax.spines[\"right\"].set_visible(False)\n",
        "    ax.spines[\"top\"].set_visible(False)\n",
        "\n",
        "    plt.gca().set_xticks(list(range(ds[\"epoch\"].min(), ds[\"epoch\"].max() + 1)))\n",
        "\n",
        "    # if tight:\n",
        "    #     plt.tight_layout(h_pad=3)\n",
        "\n",
        "    plt.margins(margins)\n",
        "    \n",
        "def line_chart_2(dsets: list, partition: int, round_column: str, labels: list = None):\n",
        "    dsets2 = []\n",
        "    for i, s in enumerate(dsets):\n",
        "        s2 = s[s.partition == partition].copy()\n",
        "        s2[\"partition\"] =  s2[\"partition\"].apply(lambda x: f\"{x}_exp{i + 1}\")\n",
        "        dsets2.append(s2)\n",
        "        \n",
        "    all_data = pd.concat(dsets2)\n",
        "    line_chart(ds=all_data, round_column=round_column, labels=labels)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Q2rU62Wv2OE"
      },
      "source": [
        "def history_to_df(all_history):\n",
        "    data = {\"epoch\": [], \"partition\": []}\n",
        "\n",
        "    epochs = {}\n",
        "    for round, round_his in enumerate(all_history):\n",
        "        for idx, his in enumerate(round_his.items()):\n",
        "            # print(idx, his)\n",
        "            partition = his[0]\n",
        "            history = his[1]\n",
        "            keys = list(history.keys())\n",
        "            for key in keys:\n",
        "                if key not in data:\n",
        "                    data[key] = []\n",
        "\n",
        "            for i in range(len(history[keys[0]])):\n",
        "                epoch = epochs.get(partition, 1)\n",
        "                data[\"partition\"].append(partition)\n",
        "                data[\"epoch\"].append(int(epoch))\n",
        "                for key in keys:\n",
        "                    data[key].append(history[key][i])\n",
        "                epochs[partition] = epoch + 1\n",
        "\n",
        "    return pd.DataFrame(data=data)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qT8ntFczv2OF"
      },
      "source": [
        "def transfer_all(model, weights):\n",
        "    model.set_weights(weights)\n",
        "\n",
        "\n",
        "def fed_avg(weights):\n",
        "    new_weights = list()\n",
        "    for weights_list_tuple in zip(*weights):\n",
        "        new_weights.append(\n",
        "            np.array([np.array(w).mean(axis=0) for w in zip(*weights_list_tuple)])\n",
        "        )\n",
        "\n",
        "    return new_weights\n",
        "\n",
        "\n",
        "def make_weight_averager(base_weights=None):\n",
        "    buffer = []\n",
        "    partition_meta = []\n",
        "\n",
        "    def partition_trained(base_weights, weights, metadata={}):\n",
        "        partition_meta.append(metadata)\n",
        "        buffer.append(weights)\n",
        "        return base_weights\n",
        "\n",
        "    def round_end():\n",
        "        new_weights = fed_avg(buffer)\n",
        "        buffer.clear()\n",
        "        partition_meta.clear()\n",
        "        return new_weights\n",
        "\n",
        "    sn = SimpleNamespace()\n",
        "    sn.partition_trained = partition_trained\n",
        "    sn.round_end = round_end\n",
        "    return sn\n",
        "\n",
        "\n",
        "# Federated learning train algorithm\n",
        "def make_train_algo(partitions, base_weights, data_provider, model_algo):\n",
        "    data = data_provider\n",
        "    models = {}\n",
        "\n",
        "    for p in partitions:\n",
        "        models[p] = model_algo()\n",
        "        models[p].set_weights(base_weights)\n",
        "\n",
        "    def train_partition(partition, current_weights):\n",
        "        gen_scores = []\n",
        "        gen_x, gen_y = data.get_general_test_set()\n",
        "        \n",
        "        class GenvalCallback(tf.keras.callbacks.Callback):\n",
        "            def on_epoch_end(self, epoch, logs=None):\n",
        "                score = self.model.evaluate(gen_x, gen_y, verbose = 0)\n",
        "                gen_scores.append(score)\n",
        "                if logs:\n",
        "                    logs[\"gen_loss\"] = score\n",
        "                \n",
        "        model = models[partition]\n",
        "        model.set_weights(current_weights)\n",
        "        train_data, val_data = data.get_data_for_partition(partition)\n",
        "        history = model.fit(\n",
        "            train_data, epochs=EPOCHS, verbose=1, validation_data=val_data, callbacks=[GenvalCallback()]\n",
        "        )\n",
        "        results = history.history.copy()\n",
        "        results[\"gen_loss\"] = gen_scores\n",
        "        \n",
        "        return results, model.get_weights()\n",
        "\n",
        "    return train_partition\n",
        "\n",
        "        \n",
        "        \n",
        "def make_train_algo_pers4(partitions, base_weights, data_provider, model_algo):\n",
        "    data = data_provider\n",
        "    local_models = {}\n",
        "    LAYERS = 2\n",
        "\n",
        "    # for p in partitions:\n",
        "    #   models[p] = (model_algo())\n",
        "    #   models[p].set_weights(base_weights)\n",
        "\n",
        "    for p in partitions:\n",
        "        local_models[p] = model_algo()\n",
        "        local_models[p].set_weights(base_weights)\n",
        "\n",
        "    def train_partition(partition, current_weights):\n",
        "        #    model = models[partition]\n",
        "        #    model.set_weights(current_weights)\n",
        "        gen_scores = []\n",
        "        gen_x, gen_y = data.get_general_test_set()\n",
        "        \n",
        "        class GenvalCallback(tf.keras.callbacks.Callback):\n",
        "            def on_epoch_end(self, epoch, logs=None):\n",
        "                score = self.model.evaluate(gen_x, gen_y, verbose = 0)\n",
        "                gen_scores.append(score)\n",
        "                if logs:\n",
        "                    logs[\"gen_loss\"] = score\n",
        "                \n",
        "        local_model = local_models[partition]\n",
        "        local_weights = local_model.get_weights()\n",
        "        local_weights[0:LAYERS] = current_weights[0:LAYERS]\n",
        "        local_model.set_weights(local_weights)\n",
        "\n",
        "        train_data, val_data = data.get_data_for_partition(partition)\n",
        "\n",
        "        history = local_model.fit(\n",
        "            train_data, epochs=EPOCHS, verbose=1, validation_data=val_data, callbacks=[GenvalCallback()]\n",
        "        )\n",
        "\n",
        "        local_weights = local_model.get_weights()\n",
        "        \n",
        "        results = history.history.copy()\n",
        "        results[\"gen_loss\"] = gen_scores\n",
        "        \n",
        "        return results, local_model.get_weights()\n",
        "\n",
        "    return train_partition\n",
        "\n",
        "\n",
        "def train_partition(partition, data, get_model, epochs=5, verbose=0):\n",
        "    gen_scores = []\n",
        "    gen_x, gen_y = data.get_general_test_set()\n",
        "\n",
        "    class GenvalCallback(tf.keras.callbacks.Callback):\n",
        "        def on_epoch_end(self, epoch, logs=None):\n",
        "            score = self.model.evaluate(gen_x, gen_y, verbose = 0)\n",
        "            gen_scores.append(score)\n",
        "            if logs:\n",
        "                logs[\"gen_loss\"] = score\n",
        "            \n",
        "            \n",
        "    model = get_model()\n",
        "\n",
        "    train_data, val_data = data.get_data_for_partition(partition)\n",
        "    history = model.fit(train_data, epochs=epochs, verbose=1, validation_data=val_data, callbacks=[GenvalCallback()])\n",
        "    results = history.history.copy()\n",
        "    results[\"gen_loss\"] = gen_scores\n",
        "    \n",
        "    return results, model\n",
        "\n",
        "\n",
        "def make_trainer(partition_chooser, base_weights, train_algo, averager):\n",
        "    run_data = {\"rounds\": 0, \"weights\": base_weights}\n",
        "    all_histories = []\n",
        "\n",
        "    def train_round():\n",
        "        histories = {}\n",
        "\n",
        "        for p in partition_chooser():\n",
        "            # print(\"Train partition\", p)\n",
        "            current_weights = run_data[\"weights\"]\n",
        "            history, weights = train_algo(p, current_weights)\n",
        "            # Perform any required averaging when each partition is trained (async fedavg for example)\n",
        "            run_data[\"weights\"] = averager.partition_trained(current_weights, weights)\n",
        "            histories[p] = history\n",
        "\n",
        "        # Perform any required averaging for the end of a round (fedavg for example)\n",
        "        run_data[\"weights\"] = averager.round_end()\n",
        "        all_histories.append(histories)\n",
        "        run_data[\"rounds\"] = run_data[\"rounds\"] + 1\n",
        "\n",
        "    def run_rounds(num_rounds):\n",
        "        for r in range(num_rounds):\n",
        "            print(\"Training round\", r)\n",
        "            train_round()\n",
        "\n",
        "        return all_histories\n",
        "\n",
        "    sn = SimpleNamespace()\n",
        "    sn.run_rounds = run_rounds\n",
        "    sn.rounds_run = lambda: run_data[\"rounds\"] # number of rounds that were run\n",
        "    sn.global_weights = lambda: run_data[\"weights\"]\n",
        "    return sn"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4YGdq8Av2OH"
      },
      "source": [
        "def all_partitions(all_partitions):\n",
        "    def make_partitions():\n",
        "        return all_partitions\n",
        "\n",
        "    return make_partitions\n",
        "\n",
        "\n",
        "def build_trainer(\n",
        "    model_algo=model_simple_lstm,\n",
        "    data_provider=qarnot_provider,\n",
        "    train_algo_provider=make_train_algo,\n",
        "    averager=make_weight_averager,\n",
        "    partition_algo=all_partitions,\n",
        "    partition_selection=None,\n",
        "    starting_weights=None,\n",
        "):\n",
        "    data = data_provider()\n",
        "    \n",
        "    if starting_weights is None:\n",
        "        model = model_algo()\n",
        "        base_weights = model.get_weights()\n",
        "    else:\n",
        "        base_weights = starting_weights\n",
        "            \n",
        "    if partition_selection is None:\n",
        "        partitions = range(get_number_of_partitions())\n",
        "    else:\n",
        "        partitions = partition_selection\n",
        "\n",
        "    train_algo = train_algo_provider(partitions, base_weights, data, model_algo)\n",
        "    avg_algo = averager(base_weights)\n",
        "    partition_chooser = partition_algo(partitions)\n",
        "\n",
        "    return make_trainer(partition_chooser, base_weights, train_algo, avg_algo)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBWKfMYsv2N-"
      },
      "source": [
        "model = model_simple_lstm()\n",
        "data = QarnotDataReader(dataset=df_main, split_date='2020-03-08')\n",
        "\n",
        "train_data, val_data = data.get_data_for_partition(0)\n",
        "# Quick test to see if everything is working\n",
        "history = model.fit(train_data, validation_data=val_data, epochs=1)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7ET6hKqv2OI"
      },
      "source": [
        "def pd_date(date):\n",
        "    return date.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "def str_to_pddate(date_string):\n",
        "    return datetime.datetime.strptime(date_string, '%Y-%m-%d')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX7Wrohxv2OK",
        "outputId": "c36f19a2-b07b-4131-9090-a390f5fa20ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Test all the above code by training a model on a slice of data\n",
        "rounds = 1\n",
        "model_provider = model_simple_lstm\n",
        "dataset=df_main[df_main.index<\"2020-01-10\"] # slice of data to use\n",
        "split_date= \"2020-01-07\" # date of train/test split\n",
        "partition_selection = partitions # or [0,2] for example\n",
        "\n",
        "data_provider = lambda: QarnotDataReader(dataset=dataset, split_date=split_date)\n",
        "\n",
        "trainer = build_trainer(\n",
        "    model_algo=model_provider,\n",
        "    partition_selection=partition_selection,\n",
        "    train_algo_provider=make_train_algo, # federated learning\n",
        "    data_provider=data_provider,\n",
        ")\n",
        "all_history = trainer.run_rounds(rounds)\n",
        "trained_weights = trainer.global_weights"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training round 0\n",
            "34/34 [==============================] - 2s 72ms/step - loss: 0.3334 - val_loss: 1.1749\n",
            "106/106 [==============================] - 4s 37ms/step - loss: 0.6122 - val_loss: 0.1841\n",
            "106/106 [==============================] - 4s 36ms/step - loss: 0.4903 - val_loss: 0.6178\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbpWJUmSLWMu"
      },
      "source": [
        "trainweek = 2\n",
        "validateweek=1\n",
        "\n",
        "df_main.index = pd.to_datetime(df_main.index)\n",
        "start_date = df_main.index.min().to_pydatetime()\n",
        "end_date = df_main.index.max().to_pydatetime() - datetime.timedelta(7*(trainweek+validateweek))\n",
        "rolling_dates = make_daterange(start_date = start_date, end_date = end_date, freq=\"1w\")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ky5ocwhFLZRF"
      },
      "source": [
        "EPOCHS = 1 # Number of epochs to train each model before fedavg\n",
        "rounds = 1 # Number of rounds of epochs then fedavg to run\n",
        "model_provider = model_simple_lstm\n",
        "partition_selection = partitions # or [0,2] for example\n",
        "\n",
        "test_model = model_provider()\n",
        "running_weights = test_model.get_weights()\n",
        "\n",
        "# Create a dataframe that stores all results\n",
        "base_df = pd.DataFrame(columns=[\"epoch\", \"partition\", \"loss\", \"val_loss\", \"gen_loss\", \"date\"])\n",
        "\n",
        "for index, date in enumerate(rolling_dates[(trainweek+validateweek):]):\n",
        "    \n",
        "    print(\"Training for date\", date)\n",
        "    \n",
        "    ## Take 3 weeks, train on 2, validate on 1\n",
        "    start_date = date - datetime.timedelta(7*(trainweek+validateweek))\n",
        "    split_date = date - datetime.timedelta(7*validateweek)\n",
        "    # import pdb; pdb.set_trace()\n",
        "    dataset=df_main[df_main.index > pd_date(start_date)]  \n",
        "    dataset = dataset[dataset.index < pd_date(date)] # slice of data to use\n",
        "\n",
        "    data_provider = lambda: QarnotDataReader(dataset=dataset, split_date=pd_date(split_date))\n",
        "\n",
        "    trainer = build_trainer(\n",
        "        model_algo=model_provider,\n",
        "        partition_selection=partition_selection, # all_partitions\n",
        "        train_algo_provider=make_train_algo, # federated learning\n",
        "        data_provider=data_provider,\n",
        "        starting_weights = running_weights\n",
        "    )\n",
        "    \n",
        "    # Accumulate results in a dataframe\n",
        "    all_history = trainer.run_rounds(rounds)\n",
        "    df_next = history_to_df(all_history)\n",
        "    df_next[\"date\"] = date\n",
        "    base_df = base_df.append(df_next)\n",
        "    base_df.reset_index(inplace=True, drop=True)\n",
        "    \n",
        "    running_weights = trainer.global_weights()\n",
        "  \n",
        "    print()\n",
        "    \n",
        "    # You can perform another validation here by loading weights into the model\n",
        "    # test_model = model_provider()\n",
        "    # test_model.set_weights(trained_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Rh_C3VaLfDH"
      },
      "source": [
        "base_df.to_csv(\"fedavg_results.csv\", index=False)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2xU9QcDv2OT"
      },
      "source": [
        "# WIP Train the individual partitions with no federation. Note that to compare with\n",
        "# the experiments including federation above, you need to make sure total epochs, learning rates\n",
        "# etc all match up for a fair comparison\n",
        "\n",
        "EPOCHS=1\n",
        "model_provider = model_simple_lstm\n",
        "\n",
        "part_df = pd.DataFrame(columns=[\"epoch\", \"partition\", \"loss\", \"val_loss\", \"gen_loss\", \"date\"])\n",
        "\n",
        "model_providers = { 0: model_provider, 1: model_provider, 2: model_provider }\n",
        "\n",
        "for index, date in enumerate(rolling_dates[(trainweek+validateweek):]):\n",
        "    \n",
        "    print(\"Training for date\", date)\n",
        "    \n",
        "    ## Take 3 weeks, train on 2, validate on 1\n",
        "    start_date = date - datetime.timedelta(7*(trainweek+validateweek))\n",
        "    split_date = date - datetime.timedelta(7*validateweek)\n",
        "    # import pdb; pdb.set_trace()\n",
        "    dataset=df_main[df_main.index > pd_date(start_date)]  \n",
        "    dataset = dataset[dataset.index < pd_date(date)] # slice of data to use\n",
        "\n",
        "    data_provider = lambda: QarnotDataReader(dataset=dataset, split_date=split_date)\n",
        "\n",
        "    histories = []\n",
        "    for i in partitions:\n",
        "        his_i, model_i = train_partition(i, data_provider(), model_providers[i], epochs=EPOCHS)\n",
        "        model_providers[i] = lambda: model_i\n",
        "        \n",
        "        df_i = history_to_df([{i:his_i}])\n",
        "        histories.append(df_i)\n",
        "\n",
        "    df_next = pd.concat(histories)\n",
        "    df_next[\"date\"] = date\n",
        "    part_df = part_df.append(df_next)\n",
        "    part_df.reset_index(inplace=True, drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJGzPUISv2OR"
      },
      "source": [
        "part_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcIcUlk0UhOv"
      },
      "source": [
        ""
      ]
    }
  ]
}